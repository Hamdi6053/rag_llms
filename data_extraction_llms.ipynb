{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f0ab24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# --quiet parametresini kaldırdık, böylece ne olduğunu görebileceğiz\n",
    "# Temel LangChain ve yardımcı kütüphaneler (bunlar aynı kalıyor)\n",
    "!pip3 install --upgrade --quiet langchain langchain-community chromadb pypdf pandas streamlit python-dotenv\n",
    "\n",
    "# Google kütüphanesi yerine OLLAMA için GEREKLİ kütüphane\n",
    "!pip3 install --upgrade --quiet ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeba939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LangChain Modülleri ---\n",
    "\n",
    "# 1. Döküman Yükleyiciler ve Bölücüler (Bunlar aynı kalıyor)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings # <- YEREL EMBEDDING İÇİN\n",
    "\n",
    "# 4. LangChain Çalıştırma ve Şablon Modülleri (Aynı kalıyor)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field # <- Deprecation uyarısını gidermek için güncellendi\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "# from dotenv import load_dotenv <- ARTIK API ANAHTARI KULLANMADIĞIMIZ İÇİN GEREKSİZ BİR KOD PARÇASI BİZİM İÇİN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d325a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Hmm, kullanıcı “Nasılsın kralım” der. Bu esmera土耳其tlyi bir ünvan sormuş. Belki de oyun veya bir虚构角色 hakkında merak ediyor olabilir.\n",
      "\n",
      "Bu ifadeyi iki parçaya ayıralım: “nasılsın” ve “kralım”. “Nasılsın” genel bir soru, bu yüzden kullanıcı belki sohbet başlatmak istemiş. “Kralım” ise belki oyun dünyasından bir şey veya takma bir isim olabilir.\n",
      "\n",
      "Şimdi, kullanıcının Türkçe konuştuğunu varsayıyoruz çünkü “kralım” kelimesi Türkiye'de yaygın olarak kullanılıyor (“kral” = imparator). Belki de kullanıcı esmera bir oyundan veya karakterden bahsediyor. Eğer “esmerapiun” gibi bir şeyseydi…… ama bu kesin değil, sadece tahminim.\n",
      "\n",
      "Bu yüzden cevabımda iki yönlü yaklaşım kullanmalıyım: hem genel sohbet modunda olmalı hem de belki esmera kelimesini anlama adımı atlamalıyım. Eğer kullanıcı daha spesifik bir konu hakkında bilgi isterse, onu teşviq etmeliyim.\n",
      "\n",
      "Kullanıcının yaşı veya cinsiyeti hakkında bir ipucu yok ama “kralım” kelimesi genelde samimi ve dostça anlaşıldığı için olasılıklı daha genç biri. Belki de oyun fanı, özellikle esmera土耳其tlik oyunları sever.\n",
      "\n",
      "Cevabımda önce merak ettiğini işaretle etmek iyi olurdu, sonra belki daha fazla detay vermesini bekleyebilirim.\n",
      "</think>\n",
      "Merhaba kralım! 😊 Nasılsın? Bugün de devam eden sohbetimiz için bir konu var mı yoksa sadece merak ettiğin mi? Esmerapiun kelimesiyle ilgili bir şey soruyorsan buradayım, yardımcı olmaya hazırım!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"deepseek-r1\")\n",
    "llm.invoke(\"Tell me a joke about cats\")\n",
    "# 'llm' nesnesini kullanarak yerel modele isteğimizi gönderiyoruz.\n",
    "response = llm.invoke(\"Nasılsın kralım\")\n",
    "# Gelen cevabın içeriğini yazdırıyoruz.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f803067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,  Vo l. 13, No. 2, June 2025  \\nCopyright © BAJECE  ISSN: 2147-284X  http://dergipark.gov.tr/bajece \\nResearch Article \\nAbstract— Ekşi Sözlük is a widely used social network where \\nnumerous unusual events are discussed. In this context, it serves \\nas a real -time news source for emergency response teams and \\ndigital news platforms. In this study, a dataset was compiled from \\ncomments shared on the Ekşi Sözlük platform regarding the \\nKahramanmaraş eart hquake on February 6, 2023. These \\ncomments were classified into four categories: Source -Based \\nInformation, Emotional Reaction, Social Inference, and Personal \\nExperience using the Gemma2 9B (9 -billion-parameter) model, \\ndeveloped by Google with advanced natu ral language processing \\ncapabilities. A dataset of 500 comments in Excel format was \\nanalyzed, comparing the model outputs with human evaluations to \\nassess classification accuracy. For this purpose, four evaluation \\ncolumns were created for each comment base d on category \\nclassification. The consistency between model -assigned categories \\nand manually determined categories was examined using these \\ncolumns. In cases where inconsistencies were detected, the model-\\ngenerated explanations were subjected to qualitativ e evaluation. \\nModel outputs that provide satisfactory explanations are \\nconsidered acceptable, the manually classified category was \\nassigned as the final evaluation. This process systematically \\nresolved inconsistencies between model and human assessments, \\nensuring the final and validated category assignments for each \\ncomment. The highest accuracy values were observed for Social \\nInference (0.99), Source -Based Information (0.98), Personal \\nExperience (0.88), and Emotional Reaction (0.83), respectively. In \\nconclusion, this study presents a methodology for improving \\nmodel performance through human supervision, contributing to \\nthe development of strategies for disaster management and crisis \\ncommunication.   \\nIndex Terms— Natural Language Processing, Text \\nClassification, Ekşi Sözlük, Gemma2 9B \\nI. INTRODUCTION\\nATURAL DISASTERS are critical events that profoundly\\nimpact social order, where simultaneous, rapid, and \\naccurate information is of vital importance. In additi on to \\ntraditional news sources, social media platforms have emerged \\nas significant information channels during disasters. Users \\nshare their emotions and thoughts regarding such events while \\nalso contributing to the flow of disaster -related information \\nthrough social media [12]. This phenomenon holds substantial \\nvalue in understanding public reactions to disasters and \\ninforming disaster management strategies. One of the \\nfrequently used platforms for sharing extraordinary events is \\nEkşi Sözlük, where users di sseminate news, photographs, and \\nobservations related to disasters and emergencies. In this \\ncontext, emergency response teams utilize data streams \\ngenerated on social media platforms such as Ekşi Sözlük to \\nidentify affected regions and assess environmental  impacts \\n[15]. \\nSocial media data possess significant potential for enhancing \\ncrisis management during natural disasters and supporting post-\\ndisaster recovery efforts [17]. However, to effectively interpret, \\nanalyze, and utilize this data, machine learning techniques must \\nbe employed [7].  \\nIn this context, the primary aim of the study is to demonstrate \\nhow social media data can be classified using large language \\nmodels during natural disasters. The research problem focuses \\non examining the potential contribu tions of rapidly and \\nmeaningfully distinguishing social media content in times of \\ncrisis to effective disaster management. Accordingly, \\ncomments related to the February 6, 2023 Kahramanmaraş \\nEarthquake, collected from Ekşi Sözlük, were classified into \\nfour categories using the Gemma2 model. \\nThe motivation of the study stems from the lack of models \\nspecifically designed to classify Turkish social media data in \\nthe context of disasters. As a contribution to the literature, this \\nstudy offers an example of appl ying a large language model to \\nTurkish-language data and proposes a method for the automatic \\nand rapid classification of disaster-related content. \\nFinally, the classification performance of the model was \\nevaluated using various metrics, and the outputs were compared \\nwith human annotations to analyze potential inconsistencies. In \\nLarge Language Models vs. Human \\nInterpretation: Which is More Accurate in Text \\nClassification? \\nAhmet Hamdi Ozkurt, Emrah Aydemir, Yasin Sonmez \\nN \\nAhmet Hamdi Özkurt , is with Department of Management Information \\nSystem University of Sakarya, Sakarya, Turkiye,(e-mail: \\nhamdi.ozkurt@ogr.sakarya.edu.tr). \\nhttps://orcid.org/0009-0008-3220-4143 \\nEmrah Aydemir , is with with Department of Management Information \\nSystem University of Sakarya,  Sakarya, Turkiye, (e-mail: \\nemrahaydemir@sakarya.edu.tr). \\nhttps://orcid.org/0000-0002-8380-7891 \\nYasin Sönmez, is with Department of Computer Technologies University, \\nBatman, Turkiye (e-mail: yasinsonmez@batman.edu.tr). \\nhttps://orcid.org/0000-0001-9303-1735 \\nManuscript received Mar. 05, 2025; accepted Apr. 14, 2025.  \\nDOI: 10.17694/bajece.1652268 \\n174'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\ndoing so, the study demonstrates the potential of large language \\nmodels in analyzing social media data during disaster events. \\nII. LITERATURE REVIEW \\nNatural disasters are crises in whic h the need for rapid, \\naccurate, and reliable information reaches its peak. Among \\nthese, earthquakes are one of the most frequently encountered \\nand devastating natural disasters on a global scale. Due to its \\ngeological location, Turkey is situated in an ear thquake-prone \\nregion, posing a constant threat to the country. The February 6, \\n2023 Kahramanmaraş earthquake tragically reaffirmed this \\nreality. The earthquakes, with magnitudes of 7.6 and 7.7, struck \\nthe Pazarcık and Elbistan districts of Kahramanmaraş, resulting \\nin the loss of thousands of lives and leaving many individuals \\nin urgent need of assistance [1]. \\nDuring extraordinary events such as earthquakes, social \\nmedia serves as an effective channel for individuals to share \\ntheir experiences, call for help , and exchange information. \\nPlatforms like Twitter and Ekşi Sözlük are widely utilized to \\nanalyze public reactions to disasters and assess crisis \\ncommunication strategies [14]. Research conducted on these \\nplatforms provides critical insights for developing  crisis \\nmanagement strategies, enhancing societal resilience, and \\naccelerating post-disaster recovery processes. \\nIn recent years, Natural Language Processing (NLP) \\ntechniques have been increasingly used to analyze and interpret \\nsocial media data during dis asters [4]. One of the key NLP \\nmethods, sentiment analysis, helps determine the public mood \\nby classifying social media posts as positive or negative. \\nMachine learning models are commonly employed to extract \\nmeaningful patterns from large datasets and addr ess complex \\nproblems. In particular, recent advancements in NLP models \\nhave significantly improved the ability to analyze and classify \\npublic responses during disaster events [6] [7]. Models like \\nGemma2 are used for the classification and analysis of socia l \\nmedia data, with their performance evaluated using metrics \\nsuch as accuracy, precision, recall, and F1-score. These metrics \\nprovide essential indicators for assessing the classification \\nsuccess, reliability, and overall effectiveness of the model. \\nStudies in the literature suggest that machine learning and \\ndeep learning techniques demonstrate high performance in the \\nclassification and analysis of social media data. For instance, \\nVaswani et al. [16] compared traditional machine learning \\napproaches wi th Transformer -based deep learning models in \\ntext classification tasks and found that Transformer -based \\nmodels achieved significantly higher accuracy. Similarly, \\nVieweg et al. (2010) conducted a systematic review of studies \\nexamining the role of social med ia in crises following natural \\ndisasters. These studies integrated both quantitative and \\nqualitative research methods to provide a comprehensive \\nanalysis. Additionally, Lindsay [9] highlighted the functional \\nefficiency of social media tools during disaster  and crisis \\nevents, emphasizing their role in accelerating the dissemination \\nof information. \\nRecent studies highlight the effectiveness of large language \\nmodels (LLMs) in social media analysis during crises. For \\nexample, Pereira et al. [13] showed that LLM s improved the \\ncomprehensiveness of summaries in crisis communication and \\nemergency response compared to traditional methods.  \\nSimilarly, Yang et al.[18] noted that LLMs provided accurate \\npredictions and interpretable explanations in analyzing mental \\nhealth data from social media. Lastly, McDaniel et al. [10] \\ndemonstrated that integrating additional event information \\nimproved success rates in classifying social media posts in the \\nhumanitarian aid context using a zero -shot classification \\napproach. \\nIn conclus ion, existing studies support the effective use of \\nNLP and machine learning techniques in the analysis of post -\\ndisaster social media data. Text classification methods form the \\nfoundation of social media data analysis, while techniques such \\nas sentiment ana lysis have the potential to optimize response \\nprocesses by ensuring rapid and accurate access to critical \\ninformation in crisis situations. Therefore, further research in \\nthis field could provide significant contributions to crisis \\nmanagement and humanitarian aid efforts. \\nIII. METHODOLOGY \\nA. DATA COLLECTION \\nWithin the scope of this study, 7,250 comments related to the \\nFebruary 6, 2023, Kahramanmaraş earthquake were collected \\nfrom the Ekşi Sözlük platform. These comments were obtained \\nusing the Selenium library in P ython, ensuring that both \\ntimestamp and comment information were preserved. The \\nextracted data were initially stored in JSON format and \\nsubsequently converted to CSV format for further processing. \\nB. DATA PREPROCESSING \\nData preprocessing is the phase in which  raw data is \\ntransformed into a structured and interpretable format. In other \\nwords, it involves converting initial raw data into final \\nprocessed data that serves as the foundation for subsequent \\nanalyses [2]. \\nIn this study, the comments extracted in JSON format were \\ncleaned by removing punctuation marks. Comments lacking \\nsemantic coherence or consisting solely of visual content were \\neliminated from the dataset. Additionally, numerical \\nexpressions within the comments were removed. However, \\ndouble quotation marks were retained during this preprocessing \\nphase. \\nC. CLASSIFICATION \\nAfter completing the data preprocessing stage, 6,895 \\ncomments remained from the original dataset of 7,250 and were \\nsubsequently classified into four categories (see Table 1). \\nIn the classification process, Gemma2, a high -performance, \\nfast, and efficient lan guage model developed by Google, was \\nutilized. This model is commonly employed in Natural \\nLanguage Processing (NLP) tasks such as text generation and \\nclassification. Additionally, Gemma2 has 2B (2 billion \\nparameters) and 27B (27 billion parameters) variant s. The \\nprimary distinction among these models lies in their parameter \\ncount, which directly influences the model's information \\n175\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nprocessing capacity. As the number of parameters increases, the \\nmodel's ability to process and learn from data improves \\nproportionally. \\nTable I \\nDefinition and Examples of Comment Categories Used for Classification \\nCATEGORY DEFINITION EXAMPLE \\nSource-\\nBased \\nInformation \\nStatements that are \\nsupported by verifiable \\ndata, academic sources, or \\ncredible authorities, \\naiming to inform or \\nexplain objectively. \\nAccording to a 2023 \\nreport by the World \\nHealth Organization, \\nair pollution causes \\napproximately 7 \\nmillion premature \\ndeaths annually. \\nEmotional \\nResponse \\nExpressions that reflect the \\nspeaker's emotions, such as \\nanger, fear, happiness, or \\nsadness, often aiming to \\nconvey a personal or \\nsubjective response. \\nIt breaks my heart to \\nsee how little is being \\ndone to protect our \\nenvironment. \\nSocial \\nInference \\nComments that draw \\nbroader conclusions about \\nsociety, culture, or \\ncollective behavior based \\non individual observations \\nor specific events. \\nThe increasing use of \\nsocial media has \\nfundamentally \\nchanged how people \\nform and maintain \\nrelationships in \\nmodern society. \\nPersonal \\nExperience \\nNarratives or statements \\nbased on the individual's \\nown life, experiences, or \\nobservations, usually \\nshared in a subjective \\nmanner. \\nLast year, I \\nvolunteered at a \\nrefugee camp, and it \\ncompletely changed \\nmy perspective on \\nglobal crises. \\nThe Gemma2 model offers several advantages when \\ncompared to other existing large language models (LLMs). \\nWhile higher -parameter models such as GPT -3.5 (175B) and \\nGPT-4 (1.7T) demonstrate more advanced comprehension and \\ntext generation capabilities, their computational demands \\nsignificantly limit their usability, particularly on local \\nhardware. Although open -source alternatives like LLaMA 2 \\n(7B–70B) offer greater modifiability, they fall short of \\nachieving the same efficiency -performance balance that \\nGemma2 provides for specific tasks. \\nThe 9B parameter model was selected for this study due to \\nits balance between efficiency and computational resource \\nconsumption (e.g., processor, RAM), offering optimal \\nperformance for classification tasks. Additionally,  the open -\\naccess availability of the Gemma2 model and its comparatively \\nstronger support for the Turkish language have been key factors \\nin its selection over alternative models [11].  \\nDuring the classification process, each comment was \\nassigned '1' if it b elonged to a specific category and '0' \\notherwise. In the initial phase, the model was tasked solely with \\ndetermining the appropriate category for each comment. Upon \\ncompletion of the classification process, the annotated data was \\nexported from a CSV file t o an Excel format for further \\nanalysis. \\n \\n \\nTable II  \\nCreated Sample Excel \\nDATE COMMENT Source- Based \\nInformation \\nEmotional \\nReaction \\nSocial \\nInference \\nPersonal \\nExperience \\n06.02.2023 Oh my God, Gaziantep is shaking very badly \\nagain, we are living in hell here, it is shaking \\nlike a cradle, please make it stop. \\n \\n \\n0 \\n \\n \\n1 \\n \\n \\n0 \\n \\n \\n1 \\n07.02.2023 The environment minister said on live \\nbroadcast that we did not leave any of our \\ncitizens hungry and exposed, right, you did \\nnot leave them hungry and exposed, you left \\nthem under the rubble. \\n \\n \\n0 \\n \\n \\n1 \\n \\n \\n1 \\n \\n \\n0 \\n07.02.2023 Malatya is in a very difficult situation, the \\nfood and shelter shortage of earthquake \\nsurvivors is growing and no one is helping, \\nplease help please \\n \\n \\n0 \\n \\n \\n1 \\n \\n \\n0 \\n \\n \\n1 \\nIV. SAMPLING AND MANUAL PROCESSING \\nFrom the 6,895 classified comments, a balanced subset of \\n500 unique comments was randomly selected, consisting of 250 \\ninstances labeled as '0' and 250 instances labeled as '1' for each \\ncategory. This random selection process was carried out to \\nensure data balance and was implemented using the following \\nPython libraries and functions: \\n• Pandas: Utilized for data analysis and processing. \\n• NumPy: Used for numerical computations. \\n• Path: Employed for creating and managing file paths. \\n• Linprog and Pulp: These functions were used for solving \\nlinear programming problems. \\nSubsequently, the randomly selected comments were also \\nmanually classified following the same  methodology. \\nComments deemed meaningless, consisting only of visual \\ncontent, or containing fewer than five words were excluded \\nfrom the dataset. After this filtering process, the remaining 500 \\ncomments were reintroduced to the model for further \\nevaluation. \\nThe selection of only 500 comments from the dataset of \\n7,250 was driven by the dual aim of developing a balanced \\n176\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nclassification model and enhancing the efficiency of the manual \\nlabeling process. Ensuring an equal number of examples from \\neach category (250 labeled as ‘0’ and 250 as ‘1’) was essential \\nfor enabling the model to perform consistently across both \\nclasses and for ensuring more reliable human intervention \\nduring the evaluation phase. \\n \\n \\nFig. 1. Flow Diagram of the Study \\nV. MODEL EXPLANATION \\nDuring the classification process, discrepancies and \\ninconsistencies were identified between the values assigned by \\nthe model and those assigned manually. To mitigate this iss ue, \\nan alternative approach was introduced to enhance the model’s \\nattentiveness and response generation. Specifically, \\nmodifications were made to the prompt content to ensure that \\nthe model provided more deliberate and well -considered \\nclassifications. \\nTable III \\nCategory Based Descriptions of the Model\\nExplanation Based on \\nInformation Source \\nExplanation Based on \\nEmotional Response \\nExplanation Based on Social \\nInference \\nExplanation Based on \\nPersonal Experience \\nNO because the comment \\ndoes not refer to a specific \\nevent or source. \\n \\n \\n \\nYES because the comment \\nreflects feelings of sadness and \\nconcern, with phrases such as \\n“they were in great shock” and \\n“I hope our loss is not too \\ngreat”. \\nYES because the comment \\ncontains a call for social support \\nand solidarity for the earthquake \\nvictims by saying “get well soon \\nto the people in that region”. \\nYES because the author's \\nstatement in the comment that \\n“I talked to my loved ones in \\nAntep and Nizip” reflects his \\npersonal experience. \\nVI.  DETERMINATION OF FINAL GROUND TRUTH \\nVALUES \\nIn this study, four category-specific evaluation columns were \\ncreated for the 500 comments stored in Excel format. These \\nevaluation columns were u tilized to assess the accuracy of the \\nmodel outputs. The following steps were followed in the \\nevaluation process: \\n• Category-Based Consistency Check:  If there was \\nconsistency between the values assigned by the model \\nand those assigned manually, the respective \\nevaluation column was left empty. \\n• Category-Based Accuracy Assessment of Model \\nOutput: In cases where discrepancies were identified \\nbetween the mo del output and manual classification, \\nthe model’s explanations were reviewed. If the \\nexplanation was deemed satisfactory, the model \\noutput was accepted in the evaluation column. \\n177\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\n• Category-Based Determination of Final Ground \\nTruth Values:  If the model’s expl anations were \\nfound to be insufficient or unconvincing, the \\nmanually assigned classification was used as the final \\nlabel in the evaluation column. \\nAs a result of these procedures, discrepancies between the \\nmodel outputs and manual classifications were reso lved \\nthrough the evaluation columns, and the final ground truth \\nvalues were established. \\nVII. FINDINGS \\nTo evaluate the predictive performance of the model across \\nfour distinct categories ( Source-Based Information, Emotional \\nResponse, Social Inference, and Perso nal Experience ), \\ncomparisons between the model -generated outputs and \\nmanually assigned labels were analyzed. These comparisons \\nillustrate the statistical distribution of the model’s predictions \\nfor each category and the accuracy rates of these predictions.  \\nBelow, two tables containing these comparisons are presented, \\nalong with detailed explanations regarding their implications. \\n \\nTable IV \\nComparison of Model Predictions and Manual Labels \\nCategory Model \\nPrediction \\n(0) \\nModel \\nPrediction \\n(1) \\nManual \\nLabel (0) \\nManual \\nLabel (1) \\nSource- \\nBased \\nInformation \\n250 250 257 243 \\nEmotional \\nReaction \\n250 250 303 197 \\nSocial  \\nInference \\n250 250 258 242 \\nPersonal \\nExperience \\n250 250 309 191 \\n \\nTable 4 shows the statistical distribution of the predictions \\nmade by the model for the four categories (coded as 0 and 1), \\nalong with the corresponding manual labels (coded as 0 and 1). \\n \\n \\nTable V \\nEvaluation Results of Model Estimates \\n \\nCategory \\nModel \\nAcceptance 0 \\nModel \\nAcceptance 1 \\nModel \\nRejection 0 \\nModel \\nRejection 1 \\nTotal \\nPrediction \\nCorrect \\nPrediction \\n \\n Percentage \\nSource-Based \\nInformation \\n250 240 0 10 500 490 %98 \\n \\nEmotional \\nResponse \\n235 180 15 70 500 415 %83 \\nSocial \\nInference \\n248 246 2 4 500 494 %99 \\nPersonal \\nExperience \\n247 194 3 56 500 441 %88 \\nTable 5 presents the number of predictions labeled as 0 and \\n1 for both accepted and rejected instances across each category, \\nalong with the total and correctly classified predictions. For \\ninstance, in the “Social Inference” category, the model correctly \\npredicted 248 instances as 0 and 246 as 1, with only 6 \\nmisclassifications. These results provide a basis for evaluating \\nthe overall classification performance of the model and \\nunderstanding the distribution of success across categories. \\nMoreover, they serve as inpu t for the calculation of \\nclassification metrics such as accuracy, precision, recall, and \\nF1-score, which are discussed in detail in the following section. \\nA. 1. CONFUSION MATRIX ANALYSIS AND \\nCLASSIFICATION METRICS \\nThe classification performance of the model is quantitatively \\nassessed using the confusion matrix. In the confusion matrix, \\ncolumns represent the final true values, while rows represent \\nthe predicted values. In this context, the evaluation columns \\nindicate the actual values in the classification, and  the model \\noutputs represent the predicted values. The following Python \\nlibraries were used to construct the confusion matrix: \\n• Pandas: Used for data processing and reading, \\nparticularly for reading columns of data in Excel \\nformat. \\n• NumPy: Used for numerical  computations, including \\npercentage calculations in the confusion matrix. \\n• Scikit-learn: Used for generating the confusion matrix \\nand calculating classification metrics (Accuracy, \\nPrecision, Recall, F1-Score). \\n• Seaborn: Used for the visualization of the confusion \\nmatrix. \\n• Matplotlib: Used for the customization of the \\nconfusion matrix visualization. \\nThe classification metrics derived from the confusion matrix \\nare utilized to measure the classification process's performance \\nin greater detail. These metrics include: \\n• Accuracy \\n• Precision \\n• Recall \\n• F1-Score \\nTable VI \\nConfusion Matrix Structure \\n REAL POSITIVE (1) REAL NEGATIVE \\n(0) \\nPREDICTION \\nPOSITIVE (1) TP FP \\nPREDICTION \\nNEGATIVE (0) FN TN \\n \\n \\n178\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nTable VII \\nClassification Metrics and Explanations \\n \\nClassification \\nmetrics \\n \\nComputation formulas \\n \\nDefinition \\n \\nAccuracy \\n \\n𝑇𝑃 +𝑇𝑁\\n𝑇𝑃 +𝐹𝑃+𝑇𝑁+𝐹𝑁 It is the overall \\naccuracy rate of the \\nmodel's classification \\nprocess. \\n \\nPrecision \\n \\n𝑇𝑃\\n𝑇𝑃+𝐹𝑃 \\nIt represents the \\nproportion of classes \\npredicted as positive \\nby the model that are \\nactually positive. \\n \\nRecall \\n \\n𝑇𝑃\\n𝑇𝑃+𝐹𝑁 It measures how \\naccurately the model \\npredicts within the \\ntrue positives. \\n \\nF-1 Score \\n \\n \\n2∗𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑅𝑒𝑐𝑎𝑙𝑙\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 +𝑅𝑒𝑐𝑎𝑙𝑙  \\nThe harmonic mean \\nof Precision and \\nRecall is used for \\ntheir combined \\nevaluation. \\n \\n \\n \\n To evaluate the overall performance of the model, it is \\nnecessary to examine the category -specific confusion matrices \\nand classification metrics as summarized in Table 7. \\n \\n- True Positive (TP): The number of positive instances \\ncorrectly classified by the model. \\n- True Negative (TN): The number of negative instances \\ncorrectly classified by the model. \\n- False Positive (FP): The number of negative instances \\nincorrectly classified as positive by the model. \\n- False Negative (FN): The number of positive i nstances \\nincorrectly classified as negative by the model [3]. \\nThe classification metrics used for performance evaluation, \\nalong with their formulas and definitions, are provided below. \\n \\nFig. 2. Confusion Matrices for Categories \\n \\nB. CATEGORY-BASED CONFUSION MATRIX \\n1) Source Information \\nUpon examining the confusion matrix for the Source \\nInformation category presented in Figure 2(a), it has been \\nobserved that the model ma de 250 true negative (TN) and 240 \\ntrue positive (TP) predictions. Additionally, it was found that \\nthere were 0 false positive (FP) and 10 false negative (FN) \\nclassifications. This indicates that the model has not tended to \\n179\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nclassify non-source information as source information (FP=0), \\nand has correctly identified the vast majority of comments \\ncontaining source information (low FN=10). \\n2) Emotional Response \\nUpon evaluating the confusion matrix for the Emotional \\nResponse category presented in Figure 2(b), it has been \\nobserved that the model made 235 true negative (TN) and 180 \\ntrue positive (TP) predictions. Additionally, the model \\nproduced 70 false positive (FP) and 15 false negative (FN) \\npredictions. This finding suggests that the model incorrectly \\nclassified som e non -emotional response comments as \\nemotional responses (high FP=70) and was able to correctly \\nidentify a portion of the comments that contained emotional \\nresponses. \\n3) Social Inference \\nUpon examining the confusion matrix for the Social Inference \\ncategory presented in Figure 2(c), it has been observed that the \\nmodel made 248 true negative (TN) and 246 true positive (TP) \\npredictions. Moreover, there were 4 false positive (FP) and 2 \\nfalse negative (FN) classifications. In this context, the model’s \\nclassification performance in this category is quite successful, \\nas indicated by the low false positive rate (FP=4) and the high \\ntrue positive rate (TP=246). \\n4) Personal Experience \\nUpon reviewing the confusion matrix for the Personal \\nExperience category presented in Figur e 2(d), it has been \\nobserved that the model made 247 true negative (TN) and 194 \\ntrue positive (TP) predictions. Additionally, there were 56 false \\npositive (FP) and 3 false negative (FN) classifications. This \\nindicates that the model incorrectly classified some non -\\npersonal experience comments as personal experiences \\n(FP=56), but successfully identified the majority of comments \\ncontaining personal experience (low FN=3). \\n \\nTable VIII \\n Performance of Category-Based Classification Metrics \\nCategories Accuracy  Precision   Recall  F-1 \\nScore \\n \\nSource-\\nBased \\nInformation \\n0.98 0.96 1.00 0.97 \\nEmotional \\nResponse \\n0.83 0.72 0.92 0.80 \\nSocial  \\nInference \\n0.99 0.98 0.99 0.98 \\nPersonal \\nExperience \\n0.88 0.77 0.98 0.86 \\n \\n \\nC. CATEGORY-BASED CLASSIFICATION METRICS \\n1) Source Information \\nUpon examining Table 8, it can be observed that the \\nclassification metrics for the Source Information category \\ndemonstrate generally successful performance (Accuracy: 0.98, \\nRecall: 1.00, Precision: 0.96, F1 -Score: 0.97). The high recall \\nindicates that the model is able to successfully identify \\ncomments containing source information, while the high \\nprecision indicates that the model can also largely classify \\ncomments that do not contain source information correctly. \\n2) Emotional Response \\nWhen reviewing the clas sification metrics for the Emotional \\nResponse category in Table 8, it is observed that the recall value \\nis high (0.92), the accuracy value is 0.83, the precision value is \\n0.72, and the F1 -Score is 0.80. These values suggest that the \\nmodel is successful in detecting comments containing \\nemotional responses (high recall), but also incorrectly classifies \\nsome comments that do not contain emotional responses as \\nemotional (lower precision). \\n3) Social Inference \\nUpon evaluating the classification metrics for the Socia l \\nInference category presented in Table 8, it is evident that the \\nvalues for accuracy, recall, precision, and F1 -Score are very \\nhigh (Accuracy: 0.99, Recall: 0.99, Precision: 0.98, F1 -Score: \\n0.98). These values indicate that the model has very low false \\npositive (FP) and false negative (FN) rates. \\n4) Personal Experience \\nAccording to the data presented in Table 8, it is observed that \\nthe model demonstrates high performance in the Personal \\nExperience category (Accuracy: 0.88, Precision: 0.77, Recall: \\n0.98, F1-Score: 0.86). The high recall indicates that the model \\ncorrectly identifies the majority of comments containing \\npersonal experience, while the lower precision value suggests \\nthat the model also incorrectly classifies some comments that \\ndo not contain personal experience as personal experience. \\nVIII. DISCUSSION AND CONCLUSION \\nThis study aimed to automatically classify the comments on \\nEkşi Sözlük related to the 6th February 2023 Kahramanmaraş  \\nearthquake and categorize them into four distinct categories \\n(Source Information, Emotional Response, Social Inference, \\nPersonal Experience) using the Gemma2 model. This work \\nhighlights the importance of analyzing social media data in \\ncrisis situations to  obtain quick and accurate information. The \\nresults obtained indicate that the classification performance of \\nthe model varies significantly across categories. \\nThe analyses show that the model demonstrated its highest \\nclassification performance in the Socia l Inference category, \\nwhile its lowest performance was observed in the Emotional \\nResponse category. In the Source Information and Personal \\nExperience categories, the model demonstrated satisfactory \\nclassification performance with accuracy rates of 98% and \\n88%, respectively. These findings suggest that the model’s \\nclassification performance varies by category, and the \\nconfidence in the results may fluctuate based on the category. \\nWhile the accuracy rate may be very high in one category, the \\nopposite can be observed in another category. In this context, it \\nis suggested that instead of fully relying on machine learning \\nmodels, category -specific approaches may be necessary for \\ndifferent classification categories. The higher classification \\nperformance in the Social Inference category may be due to the \\nlanguage being more objective and tending to express social \\ninferences directly. Conversely, in the Emotional Response \\ncategory, the intensity of ambiguous, complex expressions \\n180'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nmight make it difficult for the model to comprehend. \\nTable 8 clearly demonstrates this situation in the \\nclassification metrics. The model exhibited the highest \\nclassification performance in the Social Inference category \\n(Accuracy: 99%), indicating that it can accurately classify texts \\ncontaining social inferences, evaluations, or general comments. \\nOn the other hand, the performance in the Emotional Response \\ncategory is the lowest (Accuracy: 83%). In the Source \\nInformation (Accuracy: 98%) and Personal Experience \\n(Accuracy: 88%) categories, the mo del performed quite well. \\nAnother reason for the low classification performance in the \\nEmotional Response category may be the differences and \\nsubjectivity of emotional expressions in the dataset. The model \\nmay struggle with categorizing different emotional tones. \\nIn conclusion, this study revealed that the classification \\nperformance of the Gemma2 model in categorizing the \\ncomments on Ekşi Sözlük regarding the 6th February \\nKahramanmaraş Earthquake varies based on the category. The \\nmodel exhibited low perform ance in the Emotional Response \\ncategory, while achieving high success in the Social Inference \\ncategory. \\nThese findings highlight important points to consider when \\ndeveloping natural language processing -based sentiment \\nanalysis and automatic text classifica tion models. To improve \\nthe model’s classification performance in the Emotional \\nResponse category, various sentiment analysis techniques, such \\nas sentiment lexicons, can be utilized, or a specialized model \\nfor this category could be trained. For example, p re-trained \\nsentiment analysis models can be used to help the model \\nunderstand complex emotional expressions. Additionally, \\nadding data from broader and diverse sources (social media, \\nnews websites, etc.) could further enhance the model’s \\nclassification performance.  \\nIn particular, for categories with lower performance, fine -\\ntuning techniques can be applied to improve the model’s \\nperformance. Fine -tuning the model on category -specific \\ndatasets may enhance classification performance for those \\nparticular categ ories. Additionally, approaches such as \\nensemble learning can leverage the strengths of different \\nmodels. For instance, combining the predictions of a sentiment \\nanalysis-focused model with the Gemma2 model for the \\nEmotional Response category could improve classification \\nperformance. By developing an optimized ensemble model for \\neach category in this manner, the overall system performance \\ncan be enhanced. \\nSome limitations of the study should be considered. Firstly, \\nthe data being collected from only a single  social media \\nplatform (Ekşi Sözlük) restricts the generalizability of the \\nfindings. Additionally, the relatively small size of the dataset \\nmay have limited the model’s classification capacity and could \\nhave affected the reliability of the results. Further more, the \\npotential presence of biases within the dataset should not be \\noverlooked. In particular, the demographic profile of Ekşi \\nSözlük users may prevent the analyzed content from fully \\nrepresenting the broader population. For these reasons, future \\nstudies should consider using larger and more diverse datasets \\nfrom various social media platforms to enhance the model's \\nperformance and strengthen the generalizability of the findings. \\nThese suggestions will contribute to the development of \\nstrategies for disaster management and crisis communication. \\nREFERENCES \\n[1] AFAD, “06 Şubat 2023 Pazarcık-Elbistan Kahramanmaraş (Mw 7.7; Mw \\n7.6) depremleri raporu,” Deprem ve Risk Azaltma Genel Müdürlüğü, \\n2023. \\n[2] Y. Argüden and B. Erşahin, Veri madenciliği: Veriden bilgiye, masraftan \\ndeğere, ARGE Danışmanlık Yayınları, 2008. \\n[3] Z. Bakan and F. Kanbay, “Makine öğrenmesi yöntemleri ile eğitim \\nbaşarısına etki eden faktörlerin modellenmesi,” İstanbul Ticaret \\nÜniversitesi Fen Bilimleri Dergisi, vol. 23, no. 45, pp. 27 –41, 2024. \\n[Online]. Available: https://doi.org/10.55071/ticaretfbd.1442084 \\n[4] G. Burel and H. Alani, “Crisis event extraction service (CREES) —\\nAutomatic detection and classification of crisis -related content on social \\nmedia,” in Proc. 15th Int. Conf. Inf. Syst. Crisis Response a nd Manage., \\n2018. \\n[5] C. Coşkun and A. Baykal, “Veri madenciliğinde sınıflandırma \\nalgoritmalarının bir örnek üzerinde karşılaştırılması,” in Akademik \\nBilişim Konferansı (AB'11) Bildirileri, 2011, pp. 51–58. \\n[6] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training \\nof deep bidirectional transformers for language understanding,” arXiv \\npreprint arXiv:1810.04805, 2019. \\n[7] M. Imran, C. Castillo, J. Lucas, P. Meier, and S. Vieweg, “AIDR: \\nArtificial intelligence for disaster response,” in Proc. 22nd Int. Conf . \\nWorld Wide Web, 2015, pp. 159–162. \\n[8] O. H. Kwon et al., “Sentiment analysis of the United States public support \\nof nuclear power on social media using large language models,” \\nRenewable and Sustainable Energy Reviews, vol. 200, 114570, 2024. \\n[Online]. Available: https://doi.org/10.1016/j.rser.2024.114570 \\n[9] B. R. Lindsay, “Social media and disasters: Recent United States \\nexperiences,” J. Contingencies Crisis Manage., vol. 19, no. 1, pp. 1 –7, \\n2011. [Online]. Available: https://doi.org/10.1111/j.1468 -\\n5973.2011.00639.x \\n[10] \\u202fE. L. McDaniel, S. Scheele, and J. Liu, “Zero-shot classification of crisis \\ntweets using instruction-finetuned large language models,” in 2024 IEEE \\nInt. Humanitarian Technol. Conf. (IHTC), Nov. 2024, pp. 1–7. \\n[11] \\u202fM. Özkan and G. Kar, “Türkçe dilinde ya zılan bilimsel metinlerin derin \\nöğrenme tekniği uygulanarak çoklu sınıflandırılması,” Mühendislik \\nBilimleri ve Tasarım Dergisi, vol. 10, no. 2, pp. 504–519, 2022. [Online]. \\nAvailable: https://doi.org/10.21923/jesd.973181 \\n[12] \\u202fL. Palen and S. B. Liu, “Citizen communications in crisis: Anticipating a \\nfuture of ICT -supported public participation,” in Proc. SIGCHI Conf. \\nHuman Factors Comput. Syst., 2007, pp. 727–736. \\n[13] \\u202fJ. Pereira, R. Lotufo, and R. Nogueira, “Large language models in \\nsummarizing social media for eme rgency management,” arXiv preprint \\narXiv:2401.03158, 2024. \\n[14] \\u202fC. Reuter and M. A. Kaufhold, “Fifteen years of social media in \\nemergencies: A retrospective review and future directions for crisis \\ninformatics,” J. Contingencies Crisis Manage., vol. 26, no. 1, pp. 41–57, \\n2018. [Online]. Available: https://doi.org/10.1111/1468-5973.12196 \\n[15] \\u202fO. Sevli and N. Kemaloğlu, “Olağandışı olaylar hakkındaki tweet’lerin \\ngerçek ve gerçek dışı olarak Google BERT modeli ile sınıflandırılması,” \\nVeri Bilimi, vol. 4, no. 1, pp. 31–37, 2021. \\n[16] \\u202fA. Vaswani et al., “Attention is all you need,” in Adv. Neural Inf. Process. \\nSyst., vol. 30, 2017. \\n[17] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen, “Microblogging \\nduring two natural hazards events: What twitter may contribute to \\nsituational awareness,” in Proc. SIGCHI Conf. Human Factors Comput. \\nSyst., 2010, pp. 1079–1088. \\n[18] K. Yang et al., “MentaLLaMA: Interpretable mental health analysis on \\nsocial media with large language models,” in Proc. ACM Web Conf. \\n2024, May 2024, pp. 4489–4500. \\n181\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-07-08T15:09:49+03:00', 'author': '-', 'moddate': '2025-07-08T15:29:31+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\n \\nBIOGRAPHIES \\nAhmet Hamdi Özkurt  is going on his \\nBachelor's degree in Management \\nInformation System in Sakarya \\nUniversity. He is a third year student. He \\ncontinues to develop himself in the field \\nof artificial intelligence, large language \\nmodels, database and mobile \\nprogramming. \\n \\n  \\nEmrah Aydemir was received the M.S. \\ndegrees in computer  teaching from the \\nUniversity of Elazig Firat, in  2012 and the \\nPh.D. degree in informatics from  Istanbul \\nUniversity, Turkey, TR, in 2017.  From \\n2012 to 2015, he was an Expert with the  \\nIstanbul Commerce University. Since \\n2021, he  has been an Associate Professor \\nwith the Management Information System, \\nSakarya University. He is the author of three books, more than \\n60 articles, and more than  40 conference presentation. His \\nresearch interests include  artificial intelligence, \\nmicrocontroller, database and software  \\n \\nYasin Sönmez  was born in Diyarbakır,  \\nTurkey in 1986. H e received the B.S. \\ndegree from the Firat University, Technical \\nEducation Faculty, Department of  \\nElectronics and Computer Education in  \\n2010, M.S. degree in computer science \\nfrom the Firat University in 2012 and Ph.D. \\ndegree department of software engineering \\nat Firat University  in 2018. His research interests include, \\nartificial intelligence, and information security. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n182\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1159468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                            chunk_overlap=200,\n",
    "                                            length_function=len,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d50a9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Yerel Ollama servisinden bir embedding fonksiyonu oluşturur ve döndürür.\n",
    "    \"\"\"\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4fa836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3489810034414459}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", \n",
    "                            embeddings=embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Amsterdam\", reference=\"coffeeshop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d862a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.18615306960721245}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluator.evaluate_strings(prediction=\"Paris\", reference=\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benzersiz kimlik (ID) oluşturmak için gerekli kütüphanemiz.\n",
    "import uuid\n",
    "# Database verktör için import ettim.\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "    \"\"\"\n",
    "    Bu fonksiyon, metin parçalarını (chunks) alır, bunları vektörlere dönüştürür \n",
    "    ve kalıcı bir Chroma veritabanına kaydeder.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. ADIM: Her bir metin parçası için benzersiz bir ID oluşturma\n",
    "    # doc.page_content (metnin kendisi) kullanılarak her bir metin için tekrarlanabilir\n",
    "    # (aynı metne her zaman aynı ID'yi veren) ve benzersiz bir kimlik (ID) oluşturulur.\n",
    "    # Bu sayede, aynı metin parçasının veritabanına tekrar eklenmesi önlenir.\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # 2. ADIM: Kopya olan metinleri ayıklama\n",
    "    # Daha önce eklenmiş ID'leri hızlıca kontrol etmek için boş bir küme (set) oluşturulur.\n",
    "    unique_ids = set()\n",
    "    # Kopyası olmayan, benzersiz metin parçalarını tutacak boş bir liste oluşturulur.\n",
    "    unique_chunks = [] \n",
    "    \n",
    "    # Her bir 'chunk' ve ona karşılık gelen 'id' üzerinde bir döngü başlatılır.\n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        # Eğer bu 'id' daha önce 'unique_ids' kümesine eklenmemişse (yani bu bir kopya değilse)...\n",
    "        if id not in unique_ids:       \n",
    "            # ...bu yeni 'id'yi kümeye ekle, böylece bir dahaki sefere kopya olduğu anlaşılır.\n",
    "            unique_ids.add(id)\n",
    "            # ...ve bu benzersiz 'chunk'ı listeye ekle.\n",
    "            unique_chunks.append(chunk) \n",
    "\n",
    "    # 3. ADIM: Vektör Veritabanını Oluşturma\n",
    "    # Ayıklanan benzersiz metin parçalarını ('unique_chunks') kullanarak bir Chroma veritabanı nesnesi oluşturur.\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        # Her bir metne karşılık gelen benzersiz ID'ler listesi.\n",
    "                                        ids=list(unique_ids),\n",
    "                                        # Metinleri sayısallaştıracak (vektöre çevirecek) olan fonksiyon.\n",
    "                                        embedding=embedding_function, \n",
    "                                        # Oluşturulan veritabanının diske (bilgisayara) kaydedileceği klasör yolu.\n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    # 4. ADIM: Veritabanını Kalıcı Olarak Kaydetme\n",
    "    # Bellekte oluşturulan veritabanını, yukarıda belirtilen 'persist_directory' yoluna kalıcı olarak kaydeder.\n",
    "    # Bu sayede programı kapatıp açtığınızda veritabanını yeniden oluşturmak zorunda kalmazsınız.\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    # 5. ADIM: Oluşturulan Nesneyi Döndürme\n",
    "    # Oluşturulan ve kullanıma hazır veritabanı nesnesini fonksiyonun çıktısı olarak döndürür.\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd339836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamdi\\AppData\\Local\\Temp\\ipykernel_43448\\1894989826.py:46: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                 embedding_function=embedding_function, \n",
    "                                 vectorstore_path=\"vectorstore_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6f8a589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2016', 'page': 3, 'creationdate': '2025-07-08T15:09:49+03:00', 'subject': 'IEEE Transactions on Magnetics', 'title': '\\uf020', 'creator': 'Microsoft® Word 2016', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'moddate': '2025-07-08T15:29:31+03:00', 'page_label': '4', 'total_pages': 9, 'author': '-'}, page_content='BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nclassification model and enhancing the efficiency of the manual \\nlabeling process. Ensuring an equal number of examples from \\neach category (250 labeled as ‘0’ and 250 as ‘1’) was essential \\nfor enabling the model to perform consistently across both \\nclasses and for ensuring more reliable human intervention \\nduring the evaluation phase. \\n \\n \\nFig. 1. Flow Diagram of the Study \\nV. MODEL EXPLANATION \\nDuring the classification process, discrepancies and \\ninconsistencies were identified between the values assigned by \\nthe model and those assigned manually. To mitigate this iss ue, \\nan alternative approach was introduced to enhance the model’s \\nattentiveness and response generation. Specifically, \\nmodifications were made to the prompt content to ensure that \\nthe model provided more deliberate and well -considered \\nclassifications. \\nTable III \\nCategory Based Descriptions of the Model\\nExplanation Based on \\nInformation Source \\nExplanation Based on \\nEmotional Response \\nExplanation Based on Social \\nInference \\nExplanation Based on \\nPersonal Experience \\nNO because the comment \\ndoes not refer to a specific \\nevent or source.'),\n",
       " Document(metadata={'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'page_label': '6', 'producer': 'Microsoft® Word 2016', 'moddate': '2025-07-08T15:29:31+03:00', 'title': '\\uf020', 'total_pages': 9, 'creationdate': '2025-07-08T15:09:49+03:00', 'creator': 'Microsoft® Word 2016', 'page': 5, 'subject': 'IEEE Transactions on Magnetics', 'author': '-'}, page_content=\"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \\n                                              \\n \\nCopyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \\nTable VII \\nClassification Metrics and Explanations \\n \\nClassification \\nmetrics \\n \\nComputation formulas \\n \\nDefinition \\n \\nAccuracy \\n \\n𝑇𝑃 +𝑇𝑁\\n𝑇𝑃 +𝐹𝑃+𝑇𝑁+𝐹𝑁 It is the overall \\naccuracy rate of the \\nmodel's classification \\nprocess. \\n \\nPrecision \\n \\n𝑇𝑃\\n𝑇𝑃+𝐹𝑃 \\nIt represents the \\nproportion of classes \\npredicted as positive \\nby the model that are \\nactually positive. \\n \\nRecall \\n \\n𝑇𝑃\\n𝑇𝑃+𝐹𝑁 It measures how \\naccurately the model \\npredicts within the \\ntrue positives. \\n \\nF-1 Score \\n \\n \\n2∗𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑅𝑒𝑐𝑎𝑙𝑙\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 +𝑅𝑒𝑐𝑎𝑙𝑙  \\nThe harmonic mean \\nof Precision and \\nRecall is used for \\ntheir combined \\nevaluation. \\n \\n \\n \\n To evaluate the overall performance of the model, it is \\nnecessary to examine the category -specific confusion matrices \\nand classification metrics as summarized in Table 7. \\n \\n- True Positive (TP): The number of positive instances \\ncorrectly classified by the model. \\n- True Negative (TN): The number of negative instances \\ncorrectly classified by the model. \\n- False Positive (FP): The number of negative instances \\nincorrectly classified as positive by the model. \\n- False Negative (FN): The number of positive i nstances\"),\n",
       " Document(metadata={'author': '-', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'creator': 'Microsoft® Word 2016', 'total_pages': 9, 'subject': 'IEEE Transactions on Magnetics', 'page': 4, 'creationdate': '2025-07-08T15:09:49+03:00', 'producer': 'Microsoft® Word 2016', 'title': '\\uf020', 'moddate': '2025-07-08T15:29:31+03:00', 'page_label': '5'}, page_content=\"confusion matrix visualization. \\nThe classification metrics derived from the confusion matrix \\nare utilized to measure the classification process's performance \\nin greater detail. These metrics include: \\n• Accuracy \\n• Precision \\n• Recall \\n• F1-Score \\nTable VI \\nConfusion Matrix Structure \\n REAL POSITIVE (1) REAL NEGATIVE \\n(0) \\nPREDICTION \\nPOSITIVE (1) TP FP \\nPREDICTION \\nNEGATIVE (0) FN TN \\n \\n \\n178\"),\n",
       " Document(metadata={'subject': 'IEEE Transactions on Magnetics', 'producer': 'Microsoft® Word 2016', 'page_label': '8', 'author': '-', 'creationdate': '2025-07-08T15:09:49+03:00', 'moddate': '2025-07-08T15:29:31+03:00', 'source': 'C:/Users/hamdi/Desktop/rag_llms/data/174-182 Large Language Models vs Human Interpretation Which is More Accurate in Text Classification.pdf', 'total_pages': 9, 'title': '\\uf020', 'page': 7, 'creator': 'Microsoft® Word 2016'}, page_content='emergencies: A retrospective review and future directions for crisis \\ninformatics,” J. Contingencies Crisis Manage., vol. 26, no. 1, pp. 41–57, \\n2018. [Online]. Available: https://doi.org/10.1111/1468-5973.12196 \\n[15] \\u202fO. Sevli and N. Kemaloğlu, “Olağandışı olaylar hakkındaki tweet’lerin \\ngerçek ve gerçek dışı olarak Google BERT modeli ile sınıflandırılması,” \\nVeri Bilimi, vol. 4, no. 1, pp. 31–37, 2021. \\n[16] \\u202fA. Vaswani et al., “Attention is all you need,” in Adv. Neural Inf. Process. \\nSyst., vol. 30, 2017. \\n[17] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen, “Microblogging \\nduring two natural hazards events: What twitter may contribute to \\nsituational awareness,” in Proc. SIGCHI Conf. Human Factors Comput. \\nSyst., 2010, pp. 1079–1088. \\n[18] K. Yang et al., “MentaLLaMA: Interpretable mental health analysis on \\nsocial media with large language models,” in Proc. ACM Web Conf. \\n2024, May 2024, pp. 4489–4500. \\n181')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks = retriever.invoke(\"What is the title of the paper?\")\n",
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d18fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51bb2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer\n",
      "the question. If you don't know the answer, say that you\n",
      "don't know. DON'T MAKE UP ANYTHING.\n",
      "\n",
      "BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \n",
      "                                              \n",
      " \n",
      "Copyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \n",
      "classification model and enhancing the efficiency of the manual \n",
      "labeling process. Ensuring an equal number of examples from \n",
      "each category (250 labeled as ‘0’ and 250 as ‘1’) was essential \n",
      "for enabling the model to perform consistently across both \n",
      "classes and for ensuring more reliable human intervention \n",
      "during the evaluation phase. \n",
      " \n",
      " \n",
      "Fig. 1. Flow Diagram of the Study \n",
      "V. MODEL EXPLANATION \n",
      "During the classification process, discrepancies and \n",
      "inconsistencies were identified between the values assigned by \n",
      "the model and those assigned manually. To mitigate this iss ue, \n",
      "an alternative approach was introduced to enhance the model’s \n",
      "attentiveness and response generation. Specifically, \n",
      "modifications were made to the prompt content to ensure that \n",
      "the model provided more deliberate and well -considered \n",
      "classifications. \n",
      "Table III \n",
      "Category Based Descriptions of the Model\n",
      "Explanation Based on \n",
      "Information Source \n",
      "Explanation Based on \n",
      "Emotional Response \n",
      "Explanation Based on Social \n",
      "Inference \n",
      "Explanation Based on \n",
      "Personal Experience \n",
      "NO because the comment \n",
      "does not refer to a specific \n",
      "event or source.\n",
      "\n",
      "---\n",
      "\n",
      "BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING,     Vol. 13, No. 2, June 2025                                              \n",
      "                                              \n",
      " \n",
      "Copyright © BAJECE                                                                ISSN: 2147-284X                                                     http://dergipark.gov.tr/bajece        \n",
      "Table VII \n",
      "Classification Metrics and Explanations \n",
      " \n",
      "Classification \n",
      "metrics \n",
      " \n",
      "Computation formulas \n",
      " \n",
      "Definition \n",
      " \n",
      "Accuracy \n",
      " \n",
      "𝑇𝑃 +𝑇𝑁\n",
      "𝑇𝑃 +𝐹𝑃+𝑇𝑁+𝐹𝑁 It is the overall \n",
      "accuracy rate of the \n",
      "model's classification \n",
      "process. \n",
      " \n",
      "Precision \n",
      " \n",
      "𝑇𝑃\n",
      "𝑇𝑃+𝐹𝑃 \n",
      "It represents the \n",
      "proportion of classes \n",
      "predicted as positive \n",
      "by the model that are \n",
      "actually positive. \n",
      " \n",
      "Recall \n",
      " \n",
      "𝑇𝑃\n",
      "𝑇𝑃+𝐹𝑁 It measures how \n",
      "accurately the model \n",
      "predicts within the \n",
      "true positives. \n",
      " \n",
      "F-1 Score \n",
      " \n",
      " \n",
      "2∗𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑅𝑒𝑐𝑎𝑙𝑙\n",
      "𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 +𝑅𝑒𝑐𝑎𝑙𝑙  \n",
      "The harmonic mean \n",
      "of Precision and \n",
      "Recall is used for \n",
      "their combined \n",
      "evaluation. \n",
      " \n",
      " \n",
      " \n",
      " To evaluate the overall performance of the model, it is \n",
      "necessary to examine the category -specific confusion matrices \n",
      "and classification metrics as summarized in Table 7. \n",
      " \n",
      "- True Positive (TP): The number of positive instances \n",
      "correctly classified by the model. \n",
      "- True Negative (TN): The number of negative instances \n",
      "correctly classified by the model. \n",
      "- False Positive (FP): The number of negative instances \n",
      "incorrectly classified as positive by the model. \n",
      "- False Negative (FN): The number of positive i nstances\n",
      "\n",
      "---\n",
      "\n",
      "confusion matrix visualization. \n",
      "The classification metrics derived from the confusion matrix \n",
      "are utilized to measure the classification process's performance \n",
      "in greater detail. These metrics include: \n",
      "• Accuracy \n",
      "• Precision \n",
      "• Recall \n",
      "• F1-Score \n",
      "Table VI \n",
      "Confusion Matrix Structure \n",
      " REAL POSITIVE (1) REAL NEGATIVE \n",
      "(0) \n",
      "PREDICTION \n",
      "POSITIVE (1) TP FP \n",
      "PREDICTION \n",
      "NEGATIVE (0) FN TN \n",
      " \n",
      " \n",
      "178\n",
      "\n",
      "---\n",
      "\n",
      "emergencies: A retrospective review and future directions for crisis \n",
      "informatics,” J. Contingencies Crisis Manage., vol. 26, no. 1, pp. 41–57, \n",
      "2018. [Online]. Available: https://doi.org/10.1111/1468-5973.12196 \n",
      "[15]  O. Sevli and N. Kemaloğlu, “Olağandışı olaylar hakkındaki tweet’lerin \n",
      "gerçek ve gerçek dışı olarak Google BERT modeli ile sınıflandırılması,” \n",
      "Veri Bilimi, vol. 4, no. 1, pp. 31–37, 2021. \n",
      "[16]  A. Vaswani et al., “Attention is all you need,” in Adv. Neural Inf. Process. \n",
      "Syst., vol. 30, 2017. \n",
      "[17] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen, “Microblogging \n",
      "during two natural hazards events: What twitter may contribute to \n",
      "situational awareness,” in Proc. SIGCHI Conf. Human Factors Comput. \n",
      "Syst., 2010, pp. 1079–1088. \n",
      "[18] K. Yang et al., “MentaLLaMA: Interpretable mental health analysis on \n",
      "social media with large language models,” in Proc. ACM Web Conf. \n",
      "2024, May 2024, pp. 4489–4500. \n",
      "181\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the title of the paper?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate context text\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the title of the paper?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b84270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to answer the user\\'s query about the title of the paper based on the provided context. Let me start by carefully examining the given text.\\n\\nThe first thing that comes to mind is looking for a clear header or footer where titles are usually placed. Scanning through each segment, I see references and tables but no explicit title mentioned here. The user might be confused if they expected the title to be in this context, so maybe they think there\\'s an error because it wasn\\'t found.\\n\\nWait, let me check again. There\\'s a mention of \"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING\" and some table titles like Table III and Table VII, but nothing about the paper\\'s title itself. The references also don\\'t include the title. Hmm, maybe the user is referring to another part of the document not included here.\\n\\nBut according to the instructions, only the provided context should be used. Since there\\'s no title in the given text, it makes sense that I can\\'t answer this. However, perhaps the user wants confirmation that a title exists elsewhere or that they made a mistake. But since the task is strict about using only the retrieved context, and assuming the user might have pasted the wrong information, I should note that.\\n\\nAlso, considering the content of the paper—tables on classification metrics and confusion matrices—it\\'s likely part of an academic article within the BAJECE journal. The absence of a title in the snippet means I can\\'t provide it here. Maybe the user needs to be reminded they\\'re only given excerpts from the paper.\\n</think>\\nThe provided context does not include the title of the paper.\\n\\nBased on the information available, we cannot determine the specific title of the paper being described.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1', 'created_at': '2025-07-11T16:49:37.1196749Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 27453469100, 'load_duration': 7238331200, 'prompt_eval_count': 1202, 'prompt_eval_duration': 1324402200, 'eval_count': 357, 'eval_duration': 18874737700}, id='run--21315d26-cac3-4c3b-abb8-778f4e2f5d13-0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "932b4a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nAlright, let me try to figure out what the title of this paper is. So, looking at the provided context, I see that it starts with \"BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING\" and mentions Table VII about classification metrics. There are also sections like V. MODEL EXPLANATION and references ending with [18]. Wait, but none of these seem to mention a specific title for the paper itself.\\n\\nHmm, maybe the title is in the reference part? Let me check each citation. The first one seems to be related to crisis informatics. Then there\\'s a Turkish study about tweets using Google BERT model. There are mentions of attention mechanisms and mental health analysis on social media. But these references might not directly give the paper\\'s title.\\n\\nAlternatively, perhaps the paper is part of a special issue or volume in that journal. The citation says Vol. 13, No. 2, June 2025. Without more information like author names or specific sections outside this context, it\\'s tricky to pinpoint the exact title. \\n\\nI think I might be missing something here. Maybe the user expects me to pull a title from the references but that doesn\\'t seem right because each reference is for another paper. The main text provided doesn\\'t mention any other publication details except the journal and volume info.\\n\\nWait, looking back at the context again: It mentions \"Table VII\" which is about classification metrics, then talks about ensuring an equal number of examples from each category during evaluation. Then there\\'s a section on model explanation where they use different bases for explanations like information source, emotional response, etc. \\n\\nBut still, no title is explicitly mentioned anywhere in the provided text. The references are numbered but their titles aren\\'t included here unless I can infer them. For example, reference [18] mentions \"MentaLLaMA: Interpretable mental health analysis on social media with large language models\" which could be a paper\\'s title, but it\\'s not clear if this is the current document.\\n\\nSo, putting it all together, without any direct mention of the paper\\'s title in the retrieved context, I can\\'t determine what it is. The best answer here would be to state that the information isn\\'t available.\\n</think>\\nThe provided context does not include the title of the paper.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1', 'created_at': '2025-07-11T16:51:56.5512415Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 39518115300, 'load_duration': 11314451300, 'prompt_eval_count': 1409, 'prompt_eval_duration': 1424615500, 'eval_count': 475, 'eval_duration': 26774003800}, id='run--3d01790e-418c-49fd-a684-456bc55f0602-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What's the title of this paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07cd356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources and reasoning.\"\"\"\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    paper_title: AnswerWithSources\n",
    "    paper_summary: AnswerWithSources\n",
    "    publication_year: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b105f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper_title': {'answer': '', 'sources': '', 'reasoning': ''}, 'paper_summary': {'answer': 'This research study aimed to automatically classify comments on Ekşi Sözlük related to the 6th February 2023 Kahramanmaraş earthquake into four categories (Source Information, Emotional Response, Social Inference, Personal Experience) using a Gemma2 model. The analyses focused on evaluating classification performance across different categories based on metrics derived from confusion matrices.', 'sources': '', 'reasoning': ''}, 'publication_year': {'answer': '2025', 'sources': 'BALKAN JOURNAL OF ELECTRICAL & COMPUTER ENGINEERING, Vol. 13, No. 2, June 2025', 'reasoning': \"The publication year is explicitly mentioned in the copyright line and table header as 'June 2025'.\"}, 'paper_authors': {'answer': '[\"Ahmet Hamdi Özkurt\", \"Emrah Aydemir\", \"Yasin Sönmez\"]', 'sources': \"\\n\\nBIOGRAPHIES \\nAhmet Hamdi Özkurt is going on his Bachelor's degree in Management Information System at Sakarya University. \\nEmrah Aydemir received M.S. and Ph.D. degrees from relevant institutions (Elazig Firat University, Istanbul University) with completion years 2012 and 2017 respectively. He has been an Associate Professor since 2021. \\nYasin Sönmez was born in 1986 and received B.S. and M.S. degrees from relevant institutions (Firat University, Technical Education Faculty) with completion years spanning 2010-2017.\", 'reasoning': \"The authors are explicitly listed in the 'BIOGRAPHIES' section at the end of the context.\"}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- BU MODELLER ZATEN TANIMLIYDI ---\n",
    "# class AnswerWithSources(BaseModel):\n",
    "#     ...\n",
    "# class ExtractedInfo(BaseModel):\n",
    "#     ...\n",
    "# --------------------------------------\n",
    "\n",
    "# 1. ADIM: Parser'ı ve Prompt Şablonunu Hazırla\n",
    "\n",
    "# JSON çıktısını, senin ExtractedInfo modeline dönüştürecek olan aracı oluştur.\n",
    "parser = JsonOutputParser(pydantic_object=ExtractedInfo)\n",
    "\n",
    "# LLM'e JSON formatında nasıl cevap vereceğini anlatan yeni bir prompt şablonu oluştur.\n",
    "JSON_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert extraction agent. From the given context, extract the requested information.\n",
    "If a piece of information is not available in the context, use an empty string \"\" for that field.\n",
    "Your output must be a valid JSON object.\n",
    "\n",
    "Here are the precise formatting instructions:\n",
    "{format_instructions}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Based on the context, extract the information for the user's query:\n",
    "QUERY: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Şablonu, parser'dan gelen format talimatlarıyla birleştir.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    template=JSON_PROMPT_TEMPLATE,\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "# 2. ADIM: Zinciri Oluştur ve Çalıştır (Sorduğun Kısım)\n",
    "\n",
    "# Artık tüm parçalar hazır olduğuna göre, montaj hattını (zinciri) kur.\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "            | parser  # LLM'den gelen metni alıp ExtractedInfo nesnesine çevirecek.\n",
    "        )\n",
    "\n",
    "# Zinciri çalıştır ve sonucu al.\n",
    "result = rag_chain.invoke(\"Give me the title, summary, publication date, and authors of the research paper.\")\n",
    "\n",
    "# Sonucu ekrana yazdır.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e191875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_summary</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>paper_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Automatic Classification of Social Media Comme...</td>\n",
       "      <td>This research aims to automatically classify c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>The paper title is mentioned at the end as par...</td>\n",
       "      <td>This summary is derived from multiple parts of...</td>\n",
       "      <td></td>\n",
       "      <td>No author names are explicitly mentioned in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning</th>\n",
       "      <td>The context ends with information about the pu...</td>\n",
       "      <td>Extracted key elements from the text to form a...</td>\n",
       "      <td></td>\n",
       "      <td>The context does not list any authors or conta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper_title  \\\n",
       "answer     Automatic Classification of Social Media Comme...   \n",
       "source     The paper title is mentioned at the end as par...   \n",
       "reasoning  The context ends with information about the pu...   \n",
       "\n",
       "                                               paper_summary publication_year  \\\n",
       "answer     This research aims to automatically classify c...                    \n",
       "source     This summary is derived from multiple parts of...                    \n",
       "reasoning  Extracted key elements from the text to form a...                    \n",
       "\n",
       "                                               paper_authors  \n",
       "answer                                                        \n",
       "source     No author names are explicitly mentioned in th...  \n",
       "reasoning  The context does not list any authors or conta...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
